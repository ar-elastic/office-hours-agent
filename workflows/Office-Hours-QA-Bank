name: Office-Hours-QA-Bank
description: Scheduled daily drive transcript ingestion. Ensure transcript file exists in month folder and follows naming convention ending in transcript.vtt. This workflow will analyze and extract QA, chunk and ingest into Elastic and Cleanup GCS (used as temp storage for file processing).
triggers:
  - type: scheduled
    with:
      every: 1440m

consts:
  splitterUrl: "Replace with your own splitter and cleanup cloud function endpoint URL here"
  appsScriptUrl: "Replace with your own Appscript endpoint URL here"
  chonkerUrl: "Replace with your own Chonker endpoint URL here"
  role: "user"

inputs:
  - name: user_question
    type: string
    required: true
    default: "Describe in detail the questions and answers in the video. Capture all the solution details. Keep the speakers and approximate timestamps for each Q/A section."

steps:
  # -----------------------------------------------------------
  # STEP 1: CHECK DRIVE (Returns list of new GCS URIs) Replace with your own randomly generated Apps Script secret_key  
  # -----------------------------------------------------------
  - name: trigger-drive
    type: http
    with:
      url: "{{ consts.appsScriptUrl }}"
      method: "POST"
      headers:
        Content-Type: "application/json"
      body: '{"secret_key":
        "XXX",
        "reset_timer": true}'

  # -----------------------------------------------------------
  # STEP 2: Lookup join index as a temp workaround to get drive location until we figure out foreach syntax at the data level
  # -----------------------------------------------------------
  - name: lookup_drive_source_folders 
    type: foreach
    foreach: "{{ steps.trigger-drive.output.data.source_folders | json}}"
    steps:
      - name: post-lookupqa-drive_source_folders
        type: http
        with:
          url: "YOUR-ELASTIC-KIBANA-PUBLIC-ENDPOINT/office_hours_lookupqa/_doc"
          method: "POST"
          headers:
            Content-Type: "application/json"
            Authorization: "ApiKey
              XXX"
          body:
            drive_subfolder: "{{steps.lookup_drive_source_folders.item}}"
            gcs_location: "{{ steps.trigger-drive.output.data.new_files[foreach.index] }}"
          timeout: "500s"

  # -----------------------------------------------------------
  # STEP 2: PROCESS FILES (Assuming transcript already exists, summarize and analyze, preserve and index timestamped data to Elastic)
  # -----------------------------------------------------------
  - name: process-transcript_items
    type: foreach
    foreach: "{{ steps.trigger-drive.output.data.new_files | json_parse}}" 
    steps:

      - name: analyze-existing-text-transcript
        type: gemini.run
        connector-id: gemini-explainer-connector
        with:
          model: 'gemini-2.5-pro'
          body: '{"contents":[{"role":"user","parts":[{"fileData":{"mimeType":"text/plain","fileUri":"{{steps.process-transcript_items.item}}"}},{"text":"{{inputs.user_question}}"}]}]}'

      - name: chonker
        type: http
        with:
          url: "{{ consts.chonkerUrl }}"
          method: "POST"
          headers:
            Content-Type: "text/plain"
          body: '{{steps.analyze-existing-text-transcript.output.completion}}'

      - name: post-chonker-chunks-iterate
        type: foreach
        foreach: "{{steps.chonker.output.data.chunks}}"
        steps:
          - name: post-chonker-chunks
            type: http
            with:
              url: "YOUR-ELASTIC-KIBANA-PUBLIC-ENDPOINT/office_hours_qa/_doc"
              method: POST
              headers: 
                Content-Type: "application/json"
                Authorization: "ApiKey XXX"
              body: 
                user: "project-ada@elastic.co"
                gcs_location: "{{ steps.process-transcript_items.item }}"
                qa: "{{steps.post-chonker-chunks-iterate.item}}" 
                semantic_qa: "{{steps.post-chonker-chunks-iterate.item}}"
              timeout: "500s"

        # 2.3 Cleanup (Deletes the text transcript file from the GCS bucket)
      - name: clean_up_file-existing-transcript
        type: http
        with:
          url: "{{ consts.splitterUrl }}"
          method: "POST"
          headers:
            Content-Type: "application/json"
          body:
            action: "cleanup"
            fileUri: "{{steps.process-transcript_items.item}}"
enabled: true
